# -*- coding: utf-8 -*-
"""CSCI218 LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RZ1MNuhaGcB9DZ_0ZsgzLzPIkK37tWTF

### Import Libraries
"""

import time
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# data loading
try:
    data, meta = arff.loadarff('Rice_Cammeo_Osmancik.arff')
    df = pd.DataFrame(data)
    df['Class'] = df['Class'].str.decode('utf-8')
except FileNotFoundError:
    print("Error: 'Rice_Cammeo_Osmancik.arff' file not found.")
    exit()

X = df.drop("Class", axis=1)
y = df["Class"]

print("Features:", X.columns)
print("Classes:", np.unique(y))

df.head()

"""### Split dataset into training (60%), validation (20%) and testing (20%)"""

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)

print(f"Training set size: {len(X_train)} samples")
print(f"Validation set size: {len(X_val)} samples")
print(f"Test set size: {len(X_test)} samples")

""" ### Feature Scaling"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("Feature scaling applied to X_train, X_val, and X_test.")
print(f"Shape of X_train_scaled: {X_train_scaled.shape}")
print(f"Shape of X_val_scaled: {X_val_scaled.shape}")
print(f"Shape of X_test_scaled: {X_test_scaled.shape}")

"""### Initialize model (baseline C = 1.0)"""

model = LogisticRegression(random_state=42, solver='liblinear')

# Train model and make predictions
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)

# Calculate accuracy
acc = accuracy_score(y_test, y_pred)

# Print the classification report and accuracy
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))
print(f'Accuracy score: {acc:.4f}')

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""### Tuning (Find the best C value)"""

c_values = np.logspace(-4, 4, 20) # Generate 20 points from 10^-4 to 10^4
accuracies = []

for c in c_values:
    # Initialize Logistic Regression model with current C value
    model_c = LogisticRegression(C=c, random_state=42, solver='liblinear')

    # Train the model on scaled training data
    model_c.fit(X_train_scaled, y_train)

    # Make predictions on scaled validation data
    y_pred_val = model_c.predict(X_val_scaled)

    # Calculate accuracy
    acc_val = accuracy_score(y_val, y_pred_val)
    accuracies.append(acc_val)

# Plotting the results
plt.figure(figsize=(8, 5))
plt.plot(c_values, accuracies, marker='o', linestyle='-')
plt.xscale('log') # Use log scale for C values for better visualization
plt.xlabel('C Value')
plt.ylabel('Validation Accuracy')
plt.title('Logistic Regression Validation Accuracy vs. C Value')
plt.grid(True)
plt.show()

# Find the C value with the highest accuracy
best_c_index = np.argmax(accuracies)
best_c = c_values[best_c_index]
best_accuracy = accuracies[best_c_index]

print(f"\nBest C value: {best_c:.4f}")
print(f"Highest Accuracy for best C: {best_accuracy:.4f}")

"""### Optimize model with the best C value"""

training_times = []
prediction_times = []
accuracy_scores = []
num_repetitions = 10

for i in range(num_repetitions):
    # Initialize model for each run to ensure independence
    model_optimized = LogisticRegression(C=best_c, random_state=42, solver='liblinear')

    # Measure training time
    start_train_time = time.time()
    model_optimized.fit(X_train_scaled, y_train)
    end_train_time = time.time()
    training_times.append(end_train_time - start_train_time)

    # Measure prediction time
    start_predict_time = time.time()
    y_pred_optimized = model_optimized.predict(X_test_scaled)
    end_predict_time = time.time()
    prediction_times.append(end_predict_time - start_predict_time)

# Calculate and print the average times
average_training_time = sum(training_times) / num_repetitions
average_prediction_time = sum(prediction_times) / num_repetitions

print(f"Average training time over {num_repetitions} repetitions: {average_training_time:.5f} seconds")
print(f"Average prediction time over {num_repetitions} repetitions: {average_prediction_time:.5f} seconds")

# Calculate accuracy for the last prediction
acc_optimized = accuracy_score(y_test, y_pred_optimized)

# Print the classification report and accuracy for the last prediction
print("\nClassification Report:")
print(classification_report(y_test, y_pred_optimized, digits=4))
print(f'Accuracy score: {acc_optimized:.4f}')

# Plot the confusion matrix for the optimized model
cm_optimized = confusion_matrix(y_test, y_pred_optimized)

plt.figure(figsize=(5, 5))
sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Optimized Model)')
plt.show()

"""### Evaluate Model Coefficients"""

print("\nModel Coefficients:")
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model_optimized.coef_[0]
})
print(coefficients.sort_values(by='Coefficient', ascending=False))

def measure_inference_time(model, X_test, runs=10):
    """
    Measure average inference time per sample.
    """
    n_samples = X_test.shape[0]
    times = []

    for _ in range(runs):
        start = time.perf_counter()
        model.predict(X_test)
        end = time.perf_counter()
        times.append((end - start) / n_samples)

    return float(np.mean(times)), times

avg_time, all_times = measure_inference_time(model_optimized, X_test_scaled, runs=10)

print("\nInference time per sample over 10 runs:")
for i, t in enumerate(all_times, 1):
    print(f"Run {i}: {t:.8f} s/sample")

print(f"Average inference time per sample: {avg_time:.8f} s/sample")

# py -m pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

print("Original TrainVal class counts:")
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nSMOTE TrainVal class counts:")
unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)
print(dict(zip(unique_smote, counts_smote)))

algo_smote = LogisticRegression(C=best_c, random_state=42, solver='liblinear')
algo_smote.fit(X_train_smote, y_train_smote)

y_pred_smote = algo_smote.predict(X_test)
acc_smote = accuracy_score(y_test, y_pred_smote)

print("\nSMOTE inclusion")
print("Test Accuracy:", acc_smote)

print("\nClassification Report:")
print(classification_report(y_test, y_pred_smote, digits=4))

# confusion matrix and eval
cm_smote = confusion_matrix(y_test, y_pred_smote)

# heatmapping
plt.figure(figsize=(6, 5))
sns.heatmap(cm_smote, annot=True, fmt='d', cmap="Blues",
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title(f"Confusion Matrix â€“ LR")
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()