# -*- coding: utf-8 -*-
"""NaiveBayes Implementation (Rice Dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qbv5z_dZQp-JJ_Mz-ggjFdKENRF-G0r5

### **Kei Fong's Naive Bayes implementation with Rice Dataset**
"""

"""Reading it"""

import pandas as pd
from scipy.io import arff

try:
    data, meta = arff.loadarff('Rice_Cammeo_Osmancik.arff')
    df = pd.DataFrame(data)
    df['Class'] = df['Class'].str.decode('utf-8')
except FileNotFoundError:
    print("Error: 'Rice_Cammeo_Osmancik.arff' file not found.")
    exit()
    
# decoding (arff file type oftenly holds data in bytes)
for col in df.select_dtypes([object]):
    df[col] = df[col].str.decode('utf-8')

df.head(10)

df.info()

"""## **Attribute Information:**
1. **Area**: Returns the number of pixels within the boundaries of the rice grain.
2. **Perimeter**: Calculates the circumference by calculating the distance between pixels around the boundaries of the rice grain.
3. **Major Axis Length**: The longest line that can be drawn on the rice grain, i.e. the main axis distance, gives.
4. **Minor Axis Length**: The shortest line that can be drawn on the rice grain, i.e. the small axis distance, gives.
5. **Eccentricity**: It measures how round the ellipse, which has the same moments as the rice grain, is.
6. **Convex Area**: Returns the pixel count of the smallest convex shell of the region formed by the rice grain.
7. **Extent**: Returns the ratio of the region formed by the rice grain to the bounding box pixels
8. **Class**: Cammeo and Osmancik.


*there are NO missing values*

# **My Observation**:
1. *objective* - create a model that classifies rice into either of the 2 classes as accurately as I can
2. *attributes*- every single one of them are continuous values that are semantic (apart from id column)

### Naive Bayes Implementation
1. Probabilistic
2. Assumes that features have no correlation to each other
3. Rare occurrences of overfitting

### **Imports**
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# these above are for text classifications, not suitable for my rice dataset which is numeric
from sklearn.naive_bayes import GaussianNB
# this above works with continuous numerical features, which is what i have

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# import seaborn to see heatmap
import seaborn as sns
import matplotlib.pyplot as plt

df.info()

X = df.drop("Class", axis=1)
y = df["Class"]

#split into data and label

# changing my label from text to numbers

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = le.fit_transform(y)

# training test split
# test set 20% of the entire dataset


X_train, X_test, y_train, y_test = train_test_split(
    X,y, test_size = 0.2, random_state=42
)

# naive bayes model
nb_model = GaussianNB()

# training
nb_model.fit(X_train, y_train)

# make predictions
y_pred = nb_model.predict(X_test)

# Evaluate accuracy

accuracy = accuracy_score(y_test, y_pred)
accuracy

# confusion matrix and heatmap
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Cammeo', 'Osmancik'],
    yticklabels = ['Cammeo', 'Osmancik']
)

plt.xlabel("Predicted Label")
plt.ylabel("Actual Label")

plt.show()

# parameter	Meaning
# annot=True	Show numbers inside cells
# fmt="d"	Integer formatting
# cmap="Blues"	Color scheme
# xticklabels	Predicted classes
# yticklabels	Actual classes

# classification report

print(classification_report(y_test, y_pred, digits=4))

"""# **My Summary**

Despite Naive Bayes characteristic of looking at each feature independantly, it managed to classify the 2 rice types quite accurately of 91%. The most confused rice is Cammeo, with 37 of it mistakenly classified as Osmancik. However, that result is not too far from the misclassification that Osmancik experienced. Concluding that Naive Bayes is a suitable model to classify this numerical dataset. Naive bayes is commonly referred to as a baseline model, should be compared with other models for more complex datasets with features that have greater correlation.

---


# **How can i improve my model?**

the nature of naive bayes is that it sees each feature independantly, it assumes. Which might classify outlier rice grains wrongly (such as larger rice grains of the same type)
"""

# showing correlation between features
plt.title("Correlation Heatmap")
sns.heatmap(X.corr(), cmap="coolwarm")
plt.show()

"""Solution for this is to remove some features that are repeated. SelectKBest can help us decide which features are worth retaining."""

from sklearn.feature_selection import SelectKBest, f_classif

# try it out w diff k values ------------------------------------------------------------------------------------
selector = SelectKBest(score_func=f_classif, k=5)
X_train_selected = selector.fit_transform(X_train,y_train)
X_test_selected = selector.transform(X_test)

nb_model.fit(X_train_selected, y_train)

# see which features are kept
scores = selector.scores_

feature_scores = pd.DataFrame({
    "Feature": X.columns,
    "Score": scores
}).sort_values(by="Score", ascending=False)

feature_scores

"""^ as shown above, minor_axis_length and extent are features that are removed.

minor_axis_length can be obtained with area and major axis_length, so its not a feature needed to be included
"""

# prediction
y_pred2 = nb_model.predict(X_test_selected)

# accuracy
new_accuracy = accuracy_score(y_test, y_pred2)
print(f"Old accuracy: {accuracy} ")
print(f"New accuracy: {new_accuracy} ")

# confusion matrix and heatmap
cm2 = confusion_matrix(y_test, y_pred2)

sns.heatmap(
    cm2,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Cammeo', 'Osmancik'],
    yticklabels = ['Cammeo', 'Osmancik']
)

plt.xlabel("Predicted Label")
plt.ylabel("Actual Label")

plt.show()

"""very very slight improvement of accuracy"""

# py -m pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

print("Original TrainVal class counts:")
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nSMOTE TrainVal class counts:")
unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)
print(dict(zip(unique_smote, counts_smote)))

algo_smote = SelectKBest(score_func=f_classif, k=5)
algo_smote.fit(X_train_smote, y_train_smote)

y_pred_smote = algo_smote.predict(X_test)
acc_smote = accuracy_score(y_test, y_pred_smote)

print("\nSMOTE inclusion")
print("Test Accuracy:", acc_smote)

print("\nClassification Report:")
print(classification_report(y_test, y_pred_smote, digits=4))

# confusion matrix and eval
cm_smote = confusion_matrix(y_test, y_pred_smote)

# heatmapping
plt.figure(figsize=(6, 5))
sns.heatmap(cm_smote, annot=True, fmt='d', cmap="Blues",
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title(f"Confusion Matrix â€“ NB")
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()