# -*- coding: utf-8 -*-
"""RiceKNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kFW3zp7sD4vOQvf8sI1OXhYxCyd3v0Yi
"""

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_recall_fscore_support
)

# data loading
try:
    data, meta = arff.loadarff('Rice_Cammeo_Osmancik.arff')
    df = pd.DataFrame(data)
    df['Class'] = df['Class'].str.decode('utf-8')
except FileNotFoundError:
    print("Error: 'Rice_Cammeo_Osmancik.arff' file not found.")
    exit()

df.head()

"""Split the features/labels (X:Input Features, y:Output label)"""
# Features/Labels
X_df = df.drop("Class", axis=1)
y_df = df["Class"]

X = X_df.values
y = y_df.values

CLASSES = np.unique(y)  # e.g., ['Cammeo', 'Osmancik']

print("Dataset shape:", df.shape)
print("Feature columns:", list(X_df.columns))
print("Classes:", CLASSES)

"""Train Test Split (80% Model Training, 20% Testing)"""

X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""Train/validation split

- Validation set = used to tune model hyperparameters (like k) without touching the test set.
- Train: To Learn Model
- Validation: Tune K
- Test: Final Evaluation
"""

X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y
)

"""Feature Scaling
- Train: Fit + Transform
- Validation/Test: Transform only
"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)
X_test  = scaler.transform(X_test)

"""Baseline KNN (Before Tuning)
- Creating a default model before tuning
- Set K = 5
- Train on training set
- Predict on validation set
- Compute Accuracy
"""

baseline_k = 5   # Default / arbitrary value
baseline_knn = KNeighborsClassifier(n_neighbors=baseline_k)

# Train on training set only
baseline_knn.fit(X_train, y_train)

# Predict validation set
y_val_pred_base = baseline_knn.predict(X_val)

# Validation accuracy
baseline_val_acc = accuracy_score(y_val, y_val_pred_base)

print("Baseline k:", baseline_k)
print("Baseline validation accuracy:", baseline_val_acc)

# Optional: Full metrics
prec, rec, f1, _ = precision_recall_fscore_support(
    y_val, y_val_pred_base, average="macro"
)

print("\nBaseline Validation Metrics:")
print("Precision:", prec)
print("Recall:", rec)
print("F1:", f1)

print("\nBaseline Classification Report:")
print()
print(classification_report(y_val, y_val_pred_base, digits=4))

"""Tuning + Timing function
- Workflow: Try k → Train → Validate → Record accuracy
- Test K values: 1, 3, 5, ...25  (odd number to avoid ties vote in binary classification)
"""

def tune_k(X_train, y_train, X_val, y_val, k_list):
    """
    Find best k using validation accuracy.
    """
    best_k = None
    best_acc = -1
    results = []

    for k in k_list:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, y_train)

        y_val_pred = knn.predict(X_val)
        acc = accuracy_score(y_val, y_val_pred)

        results.append((k, acc))

        if acc > best_acc:
            best_acc = acc
            best_k = k

    return best_k, best_acc, results

def measure_inference_time(model, X_test, runs=10):
    """
    Measure average inference time per sample.
    """
    n_samples = X_test.shape[0]
    times = []

    for _ in range(runs):
        start = time.perf_counter()
        model.predict(X_test)
        end = time.perf_counter()
        times.append((end - start) / n_samples)

    return float(np.mean(times)), times

"""Run Tuning"""

k_values = list(range(1, 26, 2))  # 1,3,5,...,25
best_k, best_val_acc, k_results = tune_k(
    X_train, y_train, X_val, y_val, k_values
)

print("\nBest k:", best_k)
print("Best validation accuracy:", best_val_acc)

# Plot k vs validation accuracy
k_vals = [k for k, _ in k_results]
acc_vals = [acc for _, acc in k_results]

plt.figure()
plt.plot(k_vals, acc_vals, marker="o")
plt.xlabel("k value")
plt.ylabel("Validation Accuracy")
plt.title("k-NN Validation Accuracy vs k")
plt.grid(True)
plt.show()

"""Train final model using best_k (train on  Train+val)
- We combine train + val because after tuning, the validation data is no longer needed. Hence, we use all available training data to build final model.
"""

# Rebuild Train+Val sets (scaled correctly)
X_trainval = np.vstack([X_train, X_val])
y_trainval = np.concatenate([y_train, y_val])

knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_trainval, y_trainval)

# Predict test
y_pred = knn.predict(X_test)

# ------------------------------
# 8) Inference timing
# ------------------------------
avg_time, all_times = measure_inference_time(knn, X_test, runs=10)

print("\nInference time per sample over 10 runs:")
for i, t in enumerate(all_times, 1):
    print(f"Run {i}: {t:.8f} s/sample")

print(f"Average inference time per sample: {avg_time:.8f} s/sample")

"""Test Metric"""

acc = accuracy_score(y_test, y_pred)
prec, rec, f1, _ = precision_recall_fscore_support(
    y_test, y_pred, average="macro"
)

print("\nTest Results:")
print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1:", f1)

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=CLASSES))

"""Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred, labels=CLASSES)

plt.figure()
plt.imshow(cm)
plt.title(f"Confusion Matrix (KNN k={best_k})")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks(range(len(CLASSES)), CLASSES, rotation=45)
plt.yticks(range(len(CLASSES)), CLASSES)

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, str(cm[i, j]), ha="center", va="center")

plt.tight_layout()
plt.show()

# py -m pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

print("Original TrainVal class counts:")
unique, counts = np.unique(y_trainval, return_counts=True)
print(dict(zip(unique, counts)))

smote = SMOTE(random_state=42)
X_trainval_smote, y_trainval_smote = smote.fit_resample(X_trainval, y_trainval)

print("\nSMOTE TrainVal class counts:")
unique_smote, counts_smote = np.unique(y_trainval_smote, return_counts=True)
print(dict(zip(unique_smote, counts_smote)))

knn_smote = KNeighborsClassifier(
    n_neighbors=best_k,
    weights="uniform",  # stay on uniform as distance seems to be not as good
    metric="euclidean"
)

knn_smote.fit(X_trainval_smote, y_trainval_smote)
y_pred_smote = knn_smote.predict(X_test)
acc_smote = accuracy_score(y_test, y_pred_smote)
prec_macro_smote, rec_macro_smote, f1_macro_smote, _ = precision_recall_fscore_support(
    y_test, y_pred_smote, average="macro"
)
prec_weighted_smote, rec_weighted_smote, f1_weighted_smote, _ = precision_recall_fscore_support(
    y_test, y_pred_smote, average="weighted"
)

print("\nSMOTE + Uniform Voting")
print("Test Accuracy:", acc_smote)

print("\nClassification Report (Test):")
print(classification_report(y_test, y_pred_smote, target_names=CLASSES, digits=4))

# Confusion Matrix
cm_smote = confusion_matrix(y_test, y_pred_smote, labels=CLASSES)

plt.figure()
plt.imshow(cm_smote)
plt.title(f"Confusion Matrix AFTER (SMOTE, k={best_k})")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks(range(len(CLASSES)), CLASSES, rotation=45)
plt.yticks(range(len(CLASSES)), CLASSES)

for i in range(cm_smote.shape[0]):
    for j in range(cm_smote.shape[1]):
        plt.text(j, i, str(cm_smote[i, j]), ha="center", va="center")

plt.tight_layout()
plt.show()