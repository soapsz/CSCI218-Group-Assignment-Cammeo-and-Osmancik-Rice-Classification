# -*- coding: utf-8 -*-
"""CSCI218_SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14HBiTrYCtZmlRdqpyn9jigp-bBqbyYEP

Import Libraries
"""

import time
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.io import arff
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance

# data loading
try:
    data, meta = arff.loadarff('Rice_Cammeo_Osmancik.arff')
    df = pd.DataFrame(data)
    df['Class'] = df['Class'].str.decode('utf-8')
except FileNotFoundError:
    print("Error: 'Rice_Cammeo_Osmancik.arff' file not found.")
    exit()

X = df.drop("Class", axis=1)
y = df["Class"]

# Print Features and Classes (Matching Logistic Regression Style)
print("Features:", X.columns)
print("Classes:", np.unique(y))
print("\nFirst 5 rows of the dataset:")
print(df.head())

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)

print(f"\nTraining set size: {len(X_train)} samples")
print(f"Validation set size: {len(X_val)} samples")
print(f"Test set size: {len(X_test)} samples")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("Feature scaling applied to X_train, X_val, and X_test.")

# --- Part 3: SVM Hyperparameter Tuning & Visualization ---
# Testing C values on a log scale to find the mathematical 'peak'
param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['rbf']}

print("\nTuning SVM Hyperparameters (C-Value)...")
grid = GridSearchCV(SVC(random_state=42), param_grid, cv=5, return_train_score=False)
grid.fit(X_train_scaled, y_train)

# Extracting data for the graph
c_values = [str(val) for val in param_grid['C']] # X-axis labels
mean_scores = grid.cv_results_['mean_test_score']
best_index = np.argmax(mean_scores)
best_c = param_grid['C'][best_index]
best_score = mean_scores[best_index]

# Plotting the Tuning Process (Matching the Random Forest style)
plt.figure(figsize=(10, 6))
plt.plot(c_values, mean_scores, marker='o', linestyle='-', color='purple', label='Validation Accuracy')
plt.plot(str(best_c), best_score, marker='o', color='red', markersize=15, label=f'Peak (C={best_c})')

plt.title('SVM Tuning: C-Parameter vs. Validation Accuracy')
plt.xlabel('C Value (Penalty Parameter)')
plt.ylabel('Validation Accuracy')
plt.grid(True)
plt.legend()
plt.show()

print(f"\nRecommended C-Value: {best_c}")
best_svm = grid.best_estimator_

num_repetitions = 10
training_times = []
prediction_times = []

for i in range(num_repetitions):
    # Training
    start_train = time.time()
    best_svm.fit(X_train_scaled, y_train)
    training_times.append(time.time() - start_train)

    # Prediction
    start_pred = time.time()
    y_pred = best_svm.predict(X_test_scaled)
    prediction_times.append(time.time() - start_pred)


# Results Summary
avg_train_time = np.mean(training_times)
avg_pred_time = np.mean(prediction_times)
acc = accuracy_score(y_test, y_pred)

print(f"\nAverage training time over {num_repetitions} repetitions: {avg_train_time:.5f} seconds")
print(f"Average prediction time over {num_repetitions} repetitions: {avg_pred_time:.5f} seconds")
print(f"Final Accuracy score: {acc:.4f}")

print("\nDetailed Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', cbar=False,
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Optimized SVM)')
plt.show()

# Permutation Importance
print("\nCalculating Feature Importance via Permutation...")
result = permutation_importance(best_svm, X_test_scaled, y_test, n_repeats=10, random_state=42)
sorted_idx = result.importances_mean.argsort()

plt.figure(figsize=(10, 6))
plt.barh(X.columns[sorted_idx], result.importances_mean[sorted_idx], color='rebeccapurple')
plt.xlabel("Importance Score")
plt.title("SVM Feature Importance (Permutation Analysis)")
plt.show()

def measure_svm_inference(model, data, runs=10):
    n_samples = data.shape[0]
    all_runs = []

    for _ in range(runs):
        start = time.perf_counter()
        model.predict(data)
        end = time.perf_counter()
        all_runs.append((end - start) / n_samples)

    return np.mean(all_runs), all_runs

avg_time, run_details = measure_svm_inference(best_svm, X_test_scaled)

print("\n--- Performance Metrics ---")
print(f"Total Samples Tested: {len(X_test)}")
print(f"Average Inference Time per Sample: {avg_time:.8f} seconds")

print("\nInference stability over 10 runs:")
for i, t in enumerate(run_details, 1):
    print(f" Run {i}: {t:.8f} s/sample")

# Summary Comparison Data
print(f"\nFinal Test Accuracy: {accuracy_score(y_test, best_svm.predict(X_test_scaled)):.4f}")

# py -m pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

print("Original TrainVal class counts:")
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nSMOTE TrainVal class counts:")
unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)
print(dict(zip(unique_smote, counts_smote)))

algo_smote = SVC(C=best_c, random_state=42)
algo_smote.fit(X_train_smote, y_train_smote)

y_pred_smote = algo_smote.predict(X_test)
acc_smote = accuracy_score(y_test, y_pred_smote)

print("\nSMOTE inclusion")
print("Test Accuracy:", acc_smote)

print("\nClassification Report:")
print(classification_report(y_test, y_pred_smote, digits=4))

# confusion matrix and eval
cm_smote = confusion_matrix(y_test, y_pred_smote)

# heatmapping
plt.figure(figsize=(6, 5))
sns.heatmap(cm_smote, annot=True, fmt='d', cmap="Blues",
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title(f"Confusion Matrix â€“ SVM")
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()