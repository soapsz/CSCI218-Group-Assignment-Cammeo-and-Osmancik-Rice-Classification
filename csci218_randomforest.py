# -*- coding: utf-8 -*-
"""CSCI218-RandomForest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ig7hXFpD2YQtknzBPYkrSUCf21UfA0hn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from scipy.io import arff
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# data loading
try:
    data, meta = arff.loadarff('Rice_Cammeo_Osmancik.arff')
    df = pd.DataFrame(data)
    df['Class'] = df['Class'].str.decode('utf-8')
except FileNotFoundError:
    print("Error: 'Rice_Cammeo_Osmancik.arff' file not found.")
    exit()

X = df.drop('Class', axis=1)
y = df['Class']
feature_names = X.columns
le = LabelEncoder()
y_encoded = le.fit_transform(y)
target_names = le.classes_

print(f"Data loaded successfully. Features: {len(feature_names)}, Samples: {len(df)}")

# train and test split (20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Class distribution check
print("Full dataset class distribution:")
print(df["Class"].value_counts())
print("\nPercentage distribution:")
print(df["Class"].value_counts(normalize=True) * 100)
print("-" * 50)

counts = df["Class"].value_counts().sort_index()

plt.figure(figsize=(6, 4))
plt.bar(counts.index, counts.values, color=['#4C72B0', '#DD8452'])
plt.title("Class Distribution (Rice Dataset)")
plt.xlabel("Rice Species")
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("\n--- Data Split Summary ---")
print(f"Training samples: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)")
print(f"Testing samples:  {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)")
print("-" * 26)

# initialise random forest (baseline @ n_estimators = 100)
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# training
print("Training Random Forest model...")
start_train = time.time()
rf.fit(X_train, y_train)
train_time = time.time() - start_train
print(f"Training Time  : {train_time:.6f} seconds")

# prediction
print("Predicting test set...")
start_pred = time.time()
y_pred = rf.predict(X_test)
pred_time = time.time() - start_pred
print(f"Prediction Time: {pred_time:.6f} seconds")

# summary section
print(f"\n--- Training Summary ---")
print(f"Training Time  : {train_time:.6f} seconds")
print(f"Prediction Time: {pred_time:.6f} seconds")
print(f"Total Samples  : {len(X_test)}")
print(f"Prediction time per Sample: {pred_time / len(X_test):.8f} seconds")

print(f"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))

param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10, 15]
}

print("Tuning hyperparameters (max_depth and min_samples_split)...")

rf_base = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(
    estimator=rf_base,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_acc = grid_search.best_score_

print(f"\nBest Parameters Found: {best_params}")
print(f"Best Cross-Validation Accuracy: {best_acc:.4f}")

rf_tuned = grid_search.best_estimator_

train_time = grid_search.refit_time_

# prediction
print("Predicting test set...")
start_pred = time.time()
y_pred = rf_tuned.predict(X_test)
pred_time = time.time() - start_pred

# summary section
print(f"\n--- Training Summary ---")
print(f"Training Time  : {train_time:.6f} seconds")
print(f"Prediction Time: {pred_time:.6f} seconds")
print(f"Total Samples  : {len(X_test)}")
print(f"Prediction time per Sample: {pred_time / len(X_test):.8f} seconds")

print(f"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))

# confusion matrix and eval
cm = confusion_matrix(y_test, y_pred)

# heatmapping
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
            xticklabels=target_names, yticklabels=target_names)
plt.title(f"Confusion Matrix – Random Forest\n(Train Time: {train_time:.4f}s)")
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# most confused pair
cm_no_diag = cm.copy().astype(float)
np.fill_diagonal(cm_no_diag, 0)
max_idx = np.unravel_index(np.argmax(cm_no_diag), cm_no_diag.shape)
i_true, j_pred = max_idx

print("\nMost Confused Pair (Random Forest)")
print(f"  True class    : {target_names[i_true]}")
print(f"  Predicted as  : {target_names[j_pred]}")
print(f"Misclassified Count: {int(cm_no_diag[i_true, j_pred])}")

# feature importance plot
importances = rf_tuned.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance (Random Forest)")
plt.bar(range(X.shape[1]), importances[indices], align="center", color="teal")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=45)
plt.ylabel("Importance Score")
plt.tight_layout()
plt.show()

print("\n--- Feature Importance Ranking ---")
for f in range(X.shape[1]):
    print(f"{f+1}. {feature_names[indices[f]]}: {importances[indices[f]]:.4f}")

def measure_inference_time(model, X_test, runs=10):
    """
    Measure average inference time per sample.
    """
    n_samples = X_test.shape[0]
    times = []

    for _ in range(runs):
        start = time.perf_counter()
        model.predict(X_test)
        end = time.perf_counter()
        times.append((end - start) / n_samples)

    return float(np.mean(times)), times

avg_time, all_times = measure_inference_time(rf_tuned, X_test, runs=10)

print("\nInference time per sample over 10 runs:")
for i, t in enumerate(all_times, 1):
    print(f"Run {i}: {t:.8f} s/sample")

print(f"Average inference time per sample: {avg_time:.8f} s/sample")

# py -m pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

print("Original TrainVal class counts:")
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nSMOTE TrainVal class counts:")
unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)
print(dict(zip(unique_smote, counts_smote)))

algo_smote = RandomForestClassifier(**best_params, random_state=42)
algo_smote.fit(X_train_smote, y_train_smote)

y_pred_smote = algo_smote.predict(X_test)
acc_smote = accuracy_score(y_test, y_pred_smote)

print("\nSMOTE inclusion")
print("Test Accuracy:", acc_smote)

print("\nClassification Report:")
print(classification_report(y_test, y_pred_smote, target_names=target_names, digits=4))

# confusion matrix and eval
cm_smote = confusion_matrix(y_test, y_pred_smote)

# heatmapping
plt.figure(figsize=(6, 5))
sns.heatmap(cm_smote, annot=True, fmt='d', cmap="Blues",
            xticklabels=target_names, yticklabels=target_names)
plt.title(f"Confusion Matrix – Random Forest\n(Train Time: {train_time:.4f}s)")
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# most confused pair
cm_smote_no_diag = cm_smote.copy().astype(float)
np.fill_diagonal(cm_smote_no_diag, 0)
max_idx = np.unravel_index(np.argmax(cm_smote_no_diag), cm_smote_no_diag.shape)
i_true, j_pred = max_idx

print("\nMost Confused Pair (Random Forest)")
print(f"  True class    : {target_names[i_true]}")
print(f"  Predicted as  : {target_names[j_pred]}")
print(f"Misclassified Count: {int(cm_smote_no_diag[i_true, j_pred])}")